#!/usr/bin/env python
import keras, tensorflow as tf, numpy as np, gym, sys, copy, argparse
import os
import collections
import tqdm
import matplotlib.pyplot as plt

# set random seed
np.random.seed(0)
tf.random.set_seed(0)

class FullyConnectedModel(keras.Model):

	def __init__(self, output_size):
		super().__init__()
		initializer = tf.keras.initializers.VarianceScaling(scale=1.0, mode="fan_avg", distribution="uniform", seed=None)

		self.dense1 = tf.keras.layers.Dense(16, activation=tf.nn.relu, kernel_initializer=initializer)
		self.dense2 = tf.keras.layers.Dense(16, activation=tf.nn.relu, kernel_initializer=initializer)
		self.dense3 = tf.keras.layers.Dense(16, activation=tf.nn.relu, kernel_initializer=initializer)
		self.output_layer = tf.keras.layers.Dense(output_size, activation=tf.identity, kernel_initializer=initializer)

	def call(self, inputs):
		x = self.dense1(inputs)
		x = self.dense2(x)
		x = self.dense3(x)
		x = self.output_layer(x)
		return x

class QNetwork():

	# This class essentially defines the network architecture.
	# The network should take in state of the world as an input,
	# and output Q values of the actions available to the agent as the output.

	def __init__(self, env, lr, logdir=None):
		# Define your network architecture here. It is also a good idea to define any training operations
		# and optimizers here, initialize your variables, or alternately compile your model here.
		pass

	def save_model_weights(self, suffix):
		# Helper function to save your model / weights.
		path = os.path.join(self.logdir, "model")
		self.model.save(path)
		return path

	def load_model(self, model_file):
		# Helper function to load an existing model.
		return keras.models.load_model(model_file)


	def load_model_weights(self, weight_file):
		# Optional Helper function to load model weights.
		pass

class Replay_Memory():

	def __init__(self, memory_size=50000, burn_in=10000):

		# The memory essentially stores transitions recorder from the agent
		# taking actions in the environment.

		# Burn in episodes define the number of episodes that are written into the memory from the
		# randomly initialized agent. Memory size is the maximum size after which old elements in the memory are replaced.
		# A simple (if not the most efficient) was to implement the memory is as a list of transitions.

		# Hint: you might find this useful:
		# 		collections.deque(maxlen=memory_size)
	 	pass

	def sample_batch(self, batch_size=32):
		# This function returns a batch of randomly sampled transitions - i.e. state, action, reward, next state, terminal flag tuples.
		# You will feed this to your model to train.
		pass

	def append(self, transition):
		# Appends transition to the memory.
		pass


class DQN_Agent():

	# In this class, we will implement functions to do the following.
	# (1) Create an instance of the Q Network class.
	# (2) Create a function that constructs a policy from the Q values predicted by the Q Network.
	#		(a) Epsilon Greedy Policy.
	# 		(b) Greedy Policy.
	# (3) Create a function to train the Q Network, by interacting with the environment.
	# (4) Create a function to test the Q Network's performance on the environment.
	# (5) Create a function for Experience Replay.

	def __init__(self, environment_name, render=False):

		# Create an instance of the network itself, as well as the memory.
		# Here is also a good place to set environmental parameters,
		# as well as training parameters - number of episodes / iterations, etc.
		pass

	def epsilon_greedy_policy(self, q_values):
		# Creating epsilon greedy probabilities to sample from.
		pass

	def greedy_policy(self, q_values):
		# Creating greedy policy for test time.
		pass

	def train(self):
		# In this function, we will train our network.

		# When use replay memory, you should interact with environment here, and store these
		# transitions to memory, while also updating your model.
		pass

	def test(self, model_file=None):
		# Evaluate the performance of your agent over 20 episodes, by calculating average cumulative rewards (returns) for the 20 episodes.
		# Here you need to interact with the environment, irrespective of whether you are using replay memory.
		pass

	def burn_in_memory(self):
		# Initialize your replay memory with a burn_in number of episodes / transitions.
		pass


# Note: if you have problems creating video captures on servers without GUI,
#       you could save and relaod model to create videos on your laptop.
def test_video(agent, env, epi):
	# Usage:
	# 	you can pass the arguments within agent.train() as:
	# 		if episode % int(self.num_episodes/3) == 0:
    #       	test_video(self, self.environment_name, episode)
	save_path = "./videos-%s-%s" % (env, epi)
	if not os.path.exists(save_path):
		os.mkdir(save_path)
	# To create video
	env = gym.wrappers.Monitor(agent.env, save_path, force=True)
	reward_total = []
	state = env.reset()
	done = False
	while not done:
		env.render()
		action = agent.epsilon_greedy_policy(state, 0.05)
		next_state, reward, done, info = env.step(action)
		state = next_state
		reward_total.append(reward)
	print("reward_total: {}".format(np.sum(reward_total)))
	agent.env.close()


def parse_arguments():
	parser = argparse.ArgumentParser(description='Deep Q Network Argument Parser')
	parser.add_argument('--env',dest='env',type=str,default='CartPole-v0')
	parser.add_argument('--render',dest='render',type=int,default=0)
	parser.add_argument('--train',dest='train',type=int,default=1)
	parser.add_argument('--model',dest='model_file',type=str)
	parser.add_argument('--lr', dest='lr', type=float, default=5e-4)
	return parser.parse_args()


def main(args):

	args = parse_arguments()
	environment_name = args.env

	# You want to create an instance of the DQN_Agent class here, and then train / test it.

if __name__ == '__main__':
	main(sys.argv)
